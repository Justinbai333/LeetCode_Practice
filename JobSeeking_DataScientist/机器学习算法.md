## 各种机器学习算法原理及其优缺点

### 决策树 Decision Tree

- **原理**：决策树是一种树形结构，每一个内部节点（internal node）代表某个属性上的判断，每一个分支（branch）代表这个属性判断的结果，最后的叶节点（leaf node）代表分类结果。

- **ID3**: 以 **熵(Entropy)** 作为判断谁是父节点的标准，追求熵的最小化。 \
  熵的定义：E(S) = &sum; (-p<sub>i</sub>log<sub>2</sub>p<sub>i</sub>) \
  Information Gain = Entropy(Before) - Entropy(After) \
  问题：熵的最小化会带来过度拟合(Overfitting)，理论上将N个样本分成N个叶节点熵最小，但是没有意义。

- **C4.5**：对ID3的优化，以信息增益率（Gain Ratio）作为判断标准。 \
  Gain Ratio = Information Gain / Split Info \
  添加了对于过度细分的惩罚。

- **CART** (Classification and Regression Tree) \
  CART的分类效果一般优于其他决策树 \
  以**Gini Index**作为优化目标，选取GINI Index小的分叉优先分叉。Regression的部分：对每个叶节点里的数据分析方差，方差小于一定程度时停止分裂。

  Gini = 1 - &sum; (p<sub>i</sub>&sup2;)

- **对于过度拟合的处理**：
  1. Pruning（剪树枝）：如果一次分叉的Gini Index的缩小小于一定值的时候，就不进行这样的分叉
  2. 使用树群（Ensemble）：如Random Forest， Gradient Boost

### 逻辑回归 Logistic Regression

- **原理：**解决分类问题，运用了回归的方法。本质是将线性回归模型产生的预测值通过一个跃阶函数转化到[0,1]区间。

- **跃阶函数的使用；** \
 Sigmoid Function: sigmoid(z) = 1 / [1 + e^(-z)] \
 相对于step function的优点：\
 1.连续性 \
 2.因为在[0,1]之间，可以理解成是P( y=1 | X )

- **逻辑回归的代价函数 Cost Function：** \
  对数损失函数 (Logarithmic Loss Function): 选择对数损失函数的原因是，如果使用传统回归模型的代价函数，既误差平方和的话，Sigmoid Function的误差平方和会是一个非凸函数，难以通过求导的方式取得极值。

- **最大似然函数 Maximum Likelihood Function：** \
  MLE：最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值(模型已知，参数未知）

  对数最大似然函数：对连乘积取对数，方便求极值

- **用梯度下降法 Gradient Descent 求代价函数的最小值：** \
  1. 在微积分中，对多元函数进行求导，把求导后的结果（各个函数的偏导数）以向量的形式表达出来就是梯度。\
  2. 从数学的角度来说，梯度的方向就是函数增长最快的方向，反之，梯度的反方向就是函数下降最快的方向。逻辑回归的代价函数是一种非线性的S型函数，所以不能用求导=0的方法求最优解，所以需要用到Gradient Descent。 \
  3. 求法，首先设置一个 / 一组初始点 x<sub>0</sub>, 然后每次下降减去步长 a（learning rate）乘以梯度(Gradient), 得到 x<sub>1</sub> =  x<sub>0</sub> - a * Gradient \
  4. 求出最大似然函数的最大值，得到的权重也就是逻辑回归的最终解。

### 人工神经网络 Artificial Neural Network

- **原理：** 使用输入层 - 隐藏层 - 输出层的结构，通过调节各层之间的函数权重来达到最优化的模型。ANN分Supervised和Unsupervised两种，Supervised给定期望输出，通过调整权重使模型预测逼近期望输出，Unsupervised给定质量的测量尺度，根据该尺度来优化权重。

- **常见拓扑结构**
  1. 单层前向神经网络
  2. 多层前向神经网络
  3. 反馈网络
  4. 随机神经网络
  5. 竞争神经网络

    [神经网络常见拓扑结构](\神经网络常见拓扑结构.webp)

- **人工神经网络的分类**
  1. 单层感知器模型

    ANN的最早应用，问题；无法处理线性不可分的变量。（Collinearity）
  2. 多层感知器模型

    添加中间层(hidden layer)，调节层之间的连接权重。 \
    问题：随着神经网络的加深，训练结果容易倾向于局部最优解，而偏离全局最优解。

  3. 反向传播BP模型

    多层感知器模型的一种。通过反向传播误差来修改层间权重，可以实现自动优化。

    问题：梯度衰减，反向传播梯度时，每传递一层，梯度衰减为原来的0.25。在层数多的情况下，梯度以指数衰减，低层次几乎收不到有效的训练信号。

  4. 深度神经网络 DNN

    使用ReLU、maxout代替Sigmoid，克服梯度衰减问题。

    引伸：高速公路网络、深度残差学习

  5. 卷积神经网络 Convolutional Neural Network

    通过在上下层中间添加卷积核作为中介**提取特征**，解决学习参数量爆炸的问题。常被应用于图片的学习中，也可以被应用于语音识别。

    卷积神经网络是一种前向神经网络，不会反向传播损失函数。（普通的全连接神经网络也是）

  6. 循环神经网络 RNN

    RNN中，神经元的输出可以在下一个时间点作用于自身，即i层神经元在t时间的输入除了i-1层的输入以外还有t-1时间在i层的输入。

    RNN的深度是时间长度，同样存在梯度弥散问题，可以通过门来解决。


- **补充**

  1. 卷积神经网络的构成

    输入层、卷积层、激活函数、池化层、全连接层

    池化层的作用：对输入的特征图进行压缩；1. 可以缩小图片尺寸，降低计算复杂度。2. 可以压缩特征，提取主要特征，防止过拟合。

    Note: 多层小卷积核替换一层大卷积核，如两层3\*3换一层5\*5，在保证相同感受野的情况下降低计算量。

  2. ReLU：max{0, x}，在非负区间梯度为常数，不存在梯度消失问题。同时可以增加模型的非线性。缺点：会让部分神经元一直处于死亡状态。

  3. Maxout：输出最大的一组激活值，是一个不固定的方程，具有可学习型。是一个分段线性函数，梯度在每一个线段上都是常数，不村在梯度消失问题。

  4. 梯度弥散 / 梯度爆炸：在深层的神经网络中，第L层计算error对其的梯度时，根据链式法则，会要用到l+1层的梯度。在这个过程中，l和l+1层之间的梯度之间的商如果大于1，则梯度以指数级增长，小于1则梯度以指数级缩小。

  5. 网络退化问题：在梯度问题解决的情况下，神经网络可以得到收敛。随着网络深度增加，网络的表现先逐渐变好至饱和，后迅速下降。

  6. 残差网络：旨在解决梯度弥散和网络退化问题，通过优化残差函数F(x) = H(x) - x 来取得恒等映射H(x) = x, 确保添加更多层的时候不会出现网络退化。同时恒等映射可以创造shortcut connections，解决梯度弥散 / 爆炸问题。

  7. 激活函数的定义：上层节点的输出于下层节点的输入之间的函数关系。

  8. 神经网络如何解决过拟合：

    1. Early Stopping：对训练时间增加限制
    2. Weight Decay：对权值进行regularization增加限制
    3. Boosting：训练多个简单的神经网络取其平均预测值
    4. Dropout：直接忽略部分hidden layer中的神经元，进行训练，用梯度下降法更新参数之后再恢复被忽略的部分。第二次忽略另一批，重复步骤。
    5. 增加样本数量
    6. 人工创造noise

### K-Means 聚类算法

- **原理：**
  1. 预先设定一个k的值，确定需要多少个分组。
  2. 随机选k个点作为初始质心。(Centroid)
  3. 对分组中的每一个样本计算与质心的距离。
  4. 通过算法算出新的质心。
  5. 计算新老质心的距离差，如果小于一个阈值则停止。

- **k-means的收敛性** \
  k-means的代价方程是SSE，具有一定会收敛的特性。

- **离群值的处理** \
  进行k-means之前应该把离群值去掉。但是由于其本身的价值，可以单独拿出来分析。

- **聚类分析中ANOVA的作用**

  ANOVA: 单因素方差分析，判断变量是否对聚类结果有贡献，方差分析检验结果越显著，变量对聚类约有影响。


### 随机森林

- **集成学习的概念：**

构建多个弱分类器，讲他们的预测结果通过一定方式集成起来，作为最终预测结果


- **原理：**

  1. 创造一个由多个决策树组成的森林
  2. 每个决策树的训练样本用Bagging (Bootstrap Aggregation，即随机可重复地挑选和原样本量一样大的样本)选出 - **此为随机森林的第一个随机性**
  3. 每个决策树使用的特征的选取：设总样本中所含特征数量为M，随机取m << M大小的子特征集，从m个特征中根据GINI Index进行分叉，并且不剪树枝（Pruning） - **此为随机森林第二个随机性**
  4. 整个森林的Prediction用每个数的Prediction一起投票


  - 随机森林是一种Ensemble Method
  - Bootstrap的统计学原理：在可重复地随机抽取的情况下，抽样样本的分布与总分布一致(Sample Distribution = Population Distribution)
  - 随机特征抽取很好地解决了样本高维度问题
  - 整个Ensemble解决了单一决策树很容易出现的过拟合问题


### 梯度提升树GBDT

- **Boosting的概念**

Boosting也是一种Ensemble Method，区别于Bagging的是，Boosting不采用并行的方式，而是在旧的决策树的预测结果的基础上改进，产生新的决策树。

- **Adaboost**
1. 初始化：最初的样本集，每个样本的权重一样（1/N）。一个弱分类器。
2. 训练完后每次逐步修改样本权重，增加旧分类器判断错误的样本权重，减少判断正确的样本权重。修改权重后的样本丢给新的弱分类器。
3. 重复12之后得到一组弱分类器，赋予预测准确率高的分类器更多的决策权重，组成Ensemble

- **Gradient Boost Decision Tree**
1. 初始化：一个弱CART树（选用的特征少），最初的样本集。
2. 把旧的弱CART树的训练残差送给新的CART树训练，产生新的弱CART树。
3. 重复12，达到残差的梯度下降。
4. 最终的OOS Ensemble预测方法为：初始弱CART树预测结果 + 残差 + 残差 + ......

核心理念还是残差的梯度递减，与Logistic Regression相同

- **Extreme Gradient Boost Decision Tree**
1. 在GBDT的基础上可以自定义loss function（GBDT固定是残差）
2. 添加了正则项，惩罚模型复杂度
3. [还是要看一下Paper](\)
